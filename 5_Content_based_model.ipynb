{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COEvQSId_gIT"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('apple-twitter-sentiment-texts.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "eb3kzwUxDF91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(data.sentiment)):\n",
        "    if data.sentiment[i] == -1:\n",
        "        data[\"sentiment\"][i] = \"negative\"\n",
        "    elif data.sentiment[i] == 0:\n",
        "        data[\"sentiment\"][i] = \"neutral\"\n",
        "    else:\n",
        "        data[\"sentiment\"][i] = \"positive\"\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "KsvjWWSkDPMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"sentiment\"].value_counts()"
      ],
      "metadata": {
        "id": "qAur3RF6F29q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data[\"sentiment\"])\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Sentiment\")\n",
        "plt.text(0.5, -0.1, 'Figure 1: Sentiment Distribution', size=12, ha='center', transform=plt.gcf().transFigure)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qvy_rSW1F52V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \" \".join(review for review in data.text)\n",
        "print (\"There are {} words in the combination of all review.\".format(len(text)))"
      ],
      "metadata": {
        "id": "S8XY7yBvGFp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a word cloud image\n",
        "wordcloud = WordCloud(background_color=\"white\").generate(text)\n",
        "\n",
        "# Display the generated image\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.text(0.5, -0.05, 'Figure 2: Word Cloud', size=12, ha='center', transform=plt.gcf().transFigure)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_7SXaf_jGIU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.iloc[1].text"
      ],
      "metadata": {
        "id": "Xp-HUq8xGjbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.iloc[0].text"
      ],
      "metadata": {
        "id": "yYa1zzYMGmmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_chars():\n",
        "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+|#[a-zA-Z]+|$[a-zA-Z]+|@[a-zA-Z]+|[,.^_$*%-;é¶¯!?:]')\n",
        "    for i in range(len(data[\"text\"])):\n",
        "        data[\"text\"][i] = pattern.sub('', data[\"text\"][i])\n",
        "remove_chars()\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "r-PkEMN1Gpt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = data.copy()\n",
        "stop = stopwords.words('english')\n",
        "data_test[\"text\"] = data_test[\"text\"].str.lower().str.split()\n",
        "data_test[\"text\"] = data_test[\"text\"].apply(lambda x: [item for item in x if item not in stop])\n",
        "data_test.head()"
      ],
      "metadata": {
        "id": "r762vXfUGy7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = data_test.assign(text=data_test.text.map(' '.join))"
      ],
      "metadata": {
        "id": "fpRnfTY3G1pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test.head()"
      ],
      "metadata": {
        "id": "mlWmidTvG2wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 500\n",
        "max_len= 20\n",
        "\n",
        "def tokenize_pad_sequences(text):\n",
        "    '''\n",
        "    This function tokenize the input text into sequnences of intergers and then\n",
        "    pad each sequence to the same length\n",
        "    '''\n",
        "    # Text tokenization\n",
        "    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    # Transforms text to a sequence of integers\n",
        "    X = tokenizer.texts_to_sequences(text)\n",
        "    # Pad sequences to the same length\n",
        "    X = pad_sequences(X, padding='post', maxlen=max_len)\n",
        "    # return sequences\n",
        "    return X, tokenizer\n",
        "\n",
        "print('Before Tokenization & Padding \\n', data_test['text'][0])\n",
        "X, tokenizer = tokenize_pad_sequences(data_test['text'])\n",
        "print('After Tokenization & Padding \\n', X[0])"
      ],
      "metadata": {
        "id": "0oW6WWBtITq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = pd.get_dummies(data_test['sentiment'])\n",
        "train_data, test_data, train_label, test_label = train_test_split(X, y, test_size=0.10, random_state=1)\n",
        "\n",
        "print('Train Set ->', train_data.shape, train_label.shape)\n",
        "print('Test Set ->', test_data.shape, test_label.shape)"
      ],
      "metadata": {
        "id": "NRJ67BgyIwIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.document_count\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, 16),\n",
        "    tf.keras.layers.LSTM(16, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(3, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "30HpWNlWIwn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
        "history = model.fit(train_data, train_label, epochs=10, validation_data=(test_data, test_label))"
      ],
      "metadata": {
        "id": "RBBWfJUCI9D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, string, num):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.text(0.5, -0.1, 'Figure ' + str(num) + ': ' + string, size=12, ha='center', transform=plt.gcf().transFigure)\n",
        "    plt.show()\n",
        "\n",
        "plot_graphs(history, \"acc\", 3)\n",
        "plot_graphs(history, \"loss\", 4)\n"
      ],
      "metadata": {
        "id": "A3r7887SI_ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "rf.fit(train_data, train_label)\n",
        "rf.score(test_data, test_label)"
      ],
      "metadata": {
        "id": "nqi5T0G7JBWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "# Assuming train_texts contains your training text data and train_labels contains your training labels\n",
        "train_texts = data['text']\n",
        "train_labels = data['sentiment']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# During training\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=max_words)\n",
        "tfidf_features_train = tfidf_vectorizer.fit_transform(train_texts)\n",
        "\n",
        "# Train RandomForestClassifier using tfidf_features_train and train_labels\n",
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "rf.fit(tfidf_features_train, train_labels)\n",
        "\n",
        "# Save the TF-IDF vectorizer and RandomForestClassifier for later use\n",
        "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
        "joblib.dump(rf, 'random_forest_model.pkl')\n",
        "\n",
        "# During prediction, load the TF-IDF vectorizer and RandomForestClassifier\n",
        "# Preprocess the text and predict as shown in the previous responses\n"
      ],
      "metadata": {
        "id": "CFpu2r9MJamV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment_rf(text):\n",
        "    # Preprocess the text\n",
        "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+|#[a-zA-Z]+|$[a-zA-Z]+|@[a-zA-Z]+|[,.^_$*%-;é¶¯!?:]', '', text)\n",
        "    text = text.lower().split()\n",
        "    text = [word for word in text if word not in stopwords.words('english')]\n",
        "    text = ' '.join(text)\n",
        "\n",
        "    # Transform text into TF-IDF features using the loaded vectorizer\n",
        "    tfidf_features = tfidf_vectorizer.transform([text])\n",
        "\n",
        "    # Predict probabilities\n",
        "    pred_prob = rf.predict_proba(tfidf_features)\n",
        "\n",
        "    # Get the index of the class with the highest probability\n",
        "    label_index = pred_prob.argmax(axis=1)[0]\n",
        "\n",
        "    labels = ['negative', 'neutral', 'positive']\n",
        "    return labels[label_index]\n",
        "\n",
        "# Example of predicting a new text\n",
        "new_text = \"Thank u !! @apple for the best product!\"\n",
        "\n",
        "# Use the loaded TF-IDF vectorizer and RandomForestClassifier\n",
        "print(predict_sentiment_rf(new_text))\n",
        "# Example of predicting a new text\n",
        "new_text = \"well your product doesn't work @apple\"\n",
        "\n",
        "# Use the loaded TF-IDF vectorizer and RandomForestClassifier\n",
        "print(predict_sentiment_rf(new_text))\n",
        "# Example of predicting a new text\n",
        "new_text = \"@apple work on your products\"\n",
        "\n",
        "# Use the loaded TF-IDF vectorizer and RandomForestClassifier\n",
        "print(predict_sentiment_rf(new_text))\n"
      ],
      "metadata": {
        "id": "hVrKkEA7OP0w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}